{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from project_dataset import load_dataset\n",
    "from code2nl.model import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_name = \"neulab/codebert-cpp\"\n",
    "    num_proc = 4\n",
    "    batch_size = 40\n",
    "    max_source_length = 512  \n",
    "    max_target_length = 153 \n",
    "    data_cols = [\"CVE ID\", \"explain\", \"func_before\"]\n",
    "    save_dir = 'tf_board'\n",
    "    epochs = 100\n",
    "    grad_acc_steps = 4\n",
    "    lr = 5e-5\n",
    "    log_freq = 10\n",
    "    local_rank = -1\n",
    "    deepspeed = None\n",
    "    fp16 = False\n",
    "    lr_warmup_steps = 200\n",
    "    weight_decay = 0.05\n",
    "    task = \"root_cause\"\n",
    "    prefix = 'neulab'\n",
    "    do_lower_case = False\n",
    "    beam_size = 10\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 3431\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 382\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 954\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds['train']\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "df_val = ds['validation']\n",
    "df_val = df_val.to_pandas()\n",
    "\n",
    "df_test = ds['test']\n",
    "df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>explain</th>\n",
       "      <th>func_before</th>\n",
       "      <th>processed_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVE-2016-2546</td>\n",
       "      <td>uses an incorrect type of mutex</td>\n",
       "      <td>static int snd_timer_user_tselect(struct file ...</td>\n",
       "      <td>static int snd_timer_user_tselect(struct file ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVE-2016-1683</td>\n",
       "      <td>mishandles namespace nodes</td>\n",
       "      <td>xsltCopyOf(xsltTransformContextPtr ctxt, xmlNo...</td>\n",
       "      <td>xsltCopyOf(xsltTransformContextPtr ctxt, xmlNo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVE-2016-3078</td>\n",
       "      <td>No boundary checking</td>\n",
       "      <td>static void php_zip_get_from(INTERNAL_FUNCTION...</td>\n",
       "      <td>static void php_zip_get_from(INTERNAL_FUNCTION...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          CVE ID                           explain  \\\n",
       "0  CVE-2016-2546  uses an incorrect type of mutex    \n",
       "1  CVE-2016-1683       mishandles namespace nodes    \n",
       "2  CVE-2016-3078              No boundary checking   \n",
       "\n",
       "                                         func_before  \\\n",
       "0  static int snd_timer_user_tselect(struct file ...   \n",
       "1  xsltCopyOf(xsltTransformContextPtr ctxt, xmlNo...   \n",
       "2  static void php_zip_get_from(INTERNAL_FUNCTION...   \n",
       "\n",
       "                                      processed_func  \n",
       "0  static int snd_timer_user_tselect(struct file ...  \n",
       "1  xsltCopyOf(xsltTransformContextPtr ctxt, xmlNo...  \n",
       "2  static void php_zip_get_from(INTERNAL_FUNCTION...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(f'tmp_data/{args.task}', exist_ok=True)\n",
    "os.makedirs(f'{args.save_dir}/{args.prefix}_{args.task}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_train['code_tokens'] = df_train.func_before.apply(lambda x: x.split())\n",
    "df_train['docstring_tokens'] = df_train.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/train.jsonl','w') as f:\n",
    "    for _, row in df_train.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "df_val['code_tokens'] = df_val.func_before.apply(lambda x: x.split())\n",
    "df_val['docstring_tokens'] = df_val.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/valid.jsonl','w') as f:\n",
    "    for _, row in df_val.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "df_test['code_tokens'] = df_test.func_before.apply(lambda x: x.split())\n",
    "df_test['docstring_tokens'] = df_test.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/test.jsonl','w') as f:\n",
    "    for _, row in df_test.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, tokenizer, args,stage=None):\n",
    "    features = []\n",
    "    for example_index, example in enumerate(examples):\n",
    "        #source\n",
    "        source_tokens = tokenizer.tokenize(example.source)[:args.max_source_length-2]\n",
    "        source_tokens =[tokenizer.cls_token]+source_tokens+[tokenizer.sep_token]\n",
    "        source_ids =  tokenizer.convert_tokens_to_ids(source_tokens) \n",
    "        source_mask = [1] * (len(source_tokens))\n",
    "        padding_length = args.max_source_length - len(source_ids)\n",
    "        source_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        source_mask+=[0]*padding_length\n",
    " \n",
    "        #target\n",
    "        if stage==\"test\":\n",
    "            target_tokens = tokenizer.tokenize(\"None\")\n",
    "        else:\n",
    "            target_tokens = tokenizer.tokenize(example.target)[:args.max_target_length-2]\n",
    "        target_tokens = [tokenizer.cls_token]+target_tokens+[tokenizer.sep_token]            \n",
    "        target_ids = tokenizer.convert_tokens_to_ids(target_tokens)\n",
    "        target_mask = [1] *len(target_ids)\n",
    "        padding_length = args.max_target_length - len(target_ids)\n",
    "        target_ids+=[tokenizer.pad_token_id]*padding_length\n",
    "        target_mask+=[0]*padding_length   \n",
    "   \n",
    "        if example_index < 5:\n",
    "            if stage=='train':\n",
    "                logger.info(\"*** Example ***\")\n",
    "                logger.info(\"idx: {}\".format(example.idx))\n",
    "\n",
    "                logger.info(\"source_tokens: {}\".format([x.replace('\\u0120','_') for x in source_tokens]))\n",
    "                logger.info(\"source_ids: {}\".format(' '.join(map(str, source_ids))))\n",
    "                logger.info(\"source_mask: {}\".format(' '.join(map(str, source_mask))))\n",
    "                \n",
    "                logger.info(\"target_tokens: {}\".format([x.replace('\\u0120','_') for x in target_tokens]))\n",
    "                logger.info(\"target_ids: {}\".format(' '.join(map(str, target_ids))))\n",
    "                logger.info(\"target_mask: {}\".format(' '.join(map(str, target_mask))))\n",
    "       \n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                 example_index,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "            )\n",
    "        )\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neulab/codebert-cpp were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "pretrained_model = args.model_name\n",
    "beam_size = args.beam_size\n",
    "output_dir = f\"results/{args.task}/neulab-12layers/\"\n",
    "source_length = args.max_source_length\n",
    "target_length = args.max_target_length\n",
    "\n",
    "config = RobertaConfig.from_pretrained(pretrained_model)\n",
    "encoder = RobertaModel.from_pretrained(pretrained_model, config = config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=12)\n",
    "model = Seq2Seq(encoder = encoder,decoder = decoder,config=config,\n",
    "                beam_size=beam_size,max_length=target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "model.load_state_dict(torch.load(Path(output_dir)/\"checkpoint-best-bleu/pytorch_model.bin\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single training/test features for a example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 example_id,\n",
    "                 source_ids,\n",
    "                 target_ids,\n",
    "                 source_mask,\n",
    "                 target_mask,\n",
    "\n",
    "    ):\n",
    "        self.example_id = example_id\n",
    "        self.source_ids = source_ids\n",
    "        self.target_ids = target_ids\n",
    "        self.source_mask = source_mask\n",
    "        self.target_mask = target_mask      \n",
    "\n",
    "class Example(object):\n",
    "    \"\"\"A single training/test example.\"\"\"\n",
    "    def __init__(self,\n",
    "                 idx,\n",
    "                 source,\n",
    "                 target,\n",
    "                 ):\n",
    "        self.idx = idx\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "\n",
    "\n",
    "def get_preds(df: pd.DataFrame):\n",
    "    ps = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        examples = [\n",
    "            Example(idx, source = row.func_before, target = row.explain)\n",
    "        ]\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples, tokenizer, args, stage='test'\n",
    "        )\n",
    "        source_ids = torch.tensor(eval_features[0].source_ids, dtype = torch.long).unsqueeze(0).to('cuda')\n",
    "        source_mask = torch.tensor(eval_features[0].source_mask, dtype = torch.long).unsqueeze(0).to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids = source_ids, source_mask = source_mask)  \n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                ps.append(text)\n",
    "    \n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007826566696166992,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 28,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 954,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e676cbfd4b2548919bfb3074b6e2b685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2771 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.6404495430037239,\n",
       " 'rouge2': 0.6151234911444683,\n",
       " 'rougeL': 0.639959010184793,\n",
       " 'rougeLsum': 0.639584382942294}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = df_test.reset_index()\n",
    "preds = get_preds(df_test)\n",
    "references = []\n",
    "for idx, row in df_test.iterrows():\n",
    "    references.append(row.explain)\n",
    "\n",
    "results = rouge.compute(predictions=preds, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: int kvm_set_msr_common(struct kvm_vcpu *vcpu, struct msr_data *msr_info) {  bool pr = false;  u32 msr = msr_info->index;  u64 data = msr_info->data;   switch (msr) {  case MSR_AMD64_NB_CFG:  case MSR_IA32_UCODE_REV:  case MSR_IA32_UCODE_WRITE:  case MSR_VM_HSAVE_PA:  case MSR_AMD64_PATCH_LOADER:  case MSR_AMD64_BU_CFG2:   break;   case MSR_EFER:   return set_efer(vcpu, data);  case MSR_K7_HWCR:   data &= ~(u64)0x40; /* ignore flush filter disable */   data &= ~(u64)0x100; /* ignore ignne emulation enable */   data &= ~(u64)0x8; /* ignore TLB cache disable */   if (data != 0) {    vcpu_unimpl(vcpu, \"unimplemented HWCR wrmsr: 0x%llx\\n\",      data);    return 1;   }   break;  case MSR_FAM10H_MMIO_CONF_BASE:   if (data != 0) {    vcpu_unimpl(vcpu, \"unimplemented MMIO_CONF_BASE wrmsr: \"      \"0x%llx\\n\", data);    return 1;   }   break;  case MSR_IA32_DEBUGCTLMSR:   if (!data) {    /* We support the non-activated case already */    break;   } else if (data & ~(DEBUGCTLMSR_LBR | DEBUGCTLMSR_BTF)) {    /* Values other than LBR and BTF are vendor-specific,     thus reserved and should throw a #GP */    return 1;   }   vcpu_unimpl(vcpu, \"%s: MSR_IA32_DEBUGCTLMSR 0x%llx, nop\\n\",     __func__, data);   break;  case 0x200 ... 0x2ff:   return set_msr_mtrr(vcpu, msr, data);  case MSR_IA32_APICBASE:   kvm_set_apic_base(vcpu, data);   break;  case APIC_BASE_MSR ... APIC_BASE_MSR + 0x3ff:   return kvm_x2apic_msr_write(vcpu, msr, data);  case MSR_IA32_TSCDEADLINE:   kvm_set_lapic_tscdeadline_msr(vcpu, data);   break;  case MSR_IA32_TSC_ADJUST:   if (guest_cpuid_has_tsc_adjust(vcpu)) {    if (!msr_info->host_initiated) {     u64 adj = data - vcpu->arch.ia32_tsc_adjust_msr;     kvm_x86_ops->adjust_tsc_offset(vcpu, adj, true);    }    vcpu->arch.ia32_tsc_adjust_msr = data;   }   break;  case MSR_IA32_MISC_ENABLE:   vcpu->arch.ia32_misc_enable_msr = data;   break;  case MSR_KVM_WALL_CLOCK_NEW:  case MSR_KVM_WALL_CLOCK:   vcpu->kvm->arch.wall_clock = data;   kvm_write_wall_clock(vcpu->kvm, data);   break;  case MSR_KVM_SYSTEM_TIME_NEW:  case MSR_KVM_SYSTEM_TIME: {   kvmclock_reset(vcpu);    vcpu->arch.time = data;   kvm_make_request(KVM_REQ_CLOCK_UPDATE, vcpu);    /* we verify if the enable bit is set... */   if (!(data & 1))    break;   /* ...but clean it before doing the actual write */  vcpu->arch.time_offset = data & ~(PAGE_MASK | 1);  vcpu->arch.time_page =  gfn_to_page(vcpu->kvm, data >> PAGE_SHIFT);  if (is_error_page(vcpu->arch.time_page))    vcpu->arch.time_page = NULL;    break;  }  case MSR_KVM_ASYNC_PF_EN:   if (kvm_pv_enable_async_pf(vcpu, data))    return 1;   break;  case MSR_KVM_STEAL_TIME:    if (unlikely(!sched_info_on()))    return 1;    if (data & KVM_STEAL_RESERVED_MASK)    return 1;    if (kvm_gfn_to_hva_cache_init(vcpu->kvm, &vcpu->arch.st.stime,        data & KVM_STEAL_VALID_BITS))    return 1;    vcpu->arch.st.msr_val = data;    if (!(data & KVM_MSR_ENABLED))    break;    vcpu->arch.st.last_steal = current->sched_info.run_delay;    preempt_disable();   accumulate_steal_time(vcpu);   preempt_enable();    kvm_make_request(KVM_REQ_STEAL_UPDATE, vcpu);    break;  case MSR_KVM_PV_EOI_EN:   if (kvm_lapic_enable_pv_eoi(vcpu, data))    return 1;   break;   case MSR_IA32_MCG_CTL:  case MSR_IA32_MCG_STATUS:  case MSR_IA32_MC0_CTL ... MSR_IA32_MC0_CTL + 4 * KVM_MAX_MCE_BANKS - 1:   return set_msr_mce(vcpu, msr, data);   /* Performance counters are not protected by a CPUID bit,   * so we should check all of them in the generic path for the sake of   * cross vendor migration.   * Writing a zero into the event select MSRs disables them,   * which we perfectly emulate ;-). Any other value should be at least   * reported, some guests depend on them.   */  case MSR_K7_EVNTSEL0:  case MSR_K7_EVNTSEL1:  case MSR_K7_EVNTSEL2:  case MSR_K7_EVNTSEL3:   if (data != 0)    vcpu_unimpl(vcpu, \"unimplemented perfctr wrmsr: \"      \"0x%x data 0x%llx\\n\", msr, data);   break;  /* at least RHEL 4 unconditionally writes to the perfctr registers,   * so we ignore writes to make it happy.   */  case MSR_K7_PERFCTR0:  case MSR_K7_PERFCTR1:  case MSR_K7_PERFCTR2:  case MSR_K7_PERFCTR3:   vcpu_unimpl(vcpu, \"unimplemented perfctr wrmsr: \"     \"0x%x data 0x%llx\\n\", msr, data);   break;  case MSR_P6_PERFCTR0:  case MSR_P6_PERFCTR1:   pr = true;  case MSR_P6_EVNTSEL0:  case MSR_P6_EVNTSEL1:   if (kvm_pmu_msr(vcpu, msr))    return kvm_pmu_set_msr(vcpu, msr, data);    if (pr || data != 0)    vcpu_unimpl(vcpu, \"disabled perfctr wrmsr: \"      \"0x%x data 0x%llx\\n\", msr, data);   break;  case MSR_K7_CLK_CTL:   /*    * Ignore all writes to this no longer documented MSR.    * Writes are only relevant for old K7 processors,    * all pre-dating SVM, but a recommended workaround from    * AMD for these chips. It is possible to specify the    * affected processor models on the command line, hence    * the need to ignore the workaround.    */   break;  case HV_X64_MSR_GUEST_OS_ID ... HV_X64_MSR_SINT15:   if (kvm_hv_msr_partition_wide(msr)) {    int r;    mutex_lock(&vcpu->kvm->lock);    r = set_msr_hyperv_pw(vcpu, msr, data);    mutex_unlock(&vcpu->kvm->lock);    return r;   } else    return set_msr_hyperv(vcpu, msr, data);   break;  case MSR_IA32_BBL_CR_CTL3:   /* Drop writes to this legacy MSR -- see rdmsr    * counterpart for further detail.    */   vcpu_unimpl(vcpu, \"ignored wrmsr: 0x%x data %llx\\n\", msr, data);   break;  case MSR_AMD64_OSVW_ID_LENGTH:   if (!guest_cpuid_has_osvw(vcpu))    return 1;   vcpu->arch.osvw.length = data;   break;  case MSR_AMD64_OSVW_STATUS:   if (!guest_cpuid_has_osvw(vcpu))    return 1;   vcpu->arch.osvw.status = data;   break;  default:   if (msr && (msr == vcpu->kvm->arch.xen_hvm_config.msr))    return xen_hvm_config(vcpu, data);   if (kvm_pmu_msr(vcpu, msr))    return kvm_pmu_set_msr(vcpu, msr, data);   if (!ignore_msrs) {    vcpu_unimpl(vcpu, \"unhandled wrmsr: 0x%x data %llx\\n\",      msr, data);    return 1;   } else {    vcpu_unimpl(vcpu, \"ignored wrmsr: 0x%x data %llx\\n\",      msr, data);    break;   }  }  return 0; } \n",
      "Original Comment: does not ensure a required time_page alignment during an MSR_KVM_SYSTEM_TIME operation \n",
      "Generated Comment: does not properly handle the writing of a non-canonical address to a model-specific register\n",
      "========================================\n",
      "Code: mobility_print(netdissect_options *ndo,  const u_char *bp, const u_char *bp2 _U_) {  const struct ip6_mobility *mh;  const u_char *ep;  unsigned mhlen, hlen;  uint8_t type;   mh = (const struct ip6_mobility *)bp;   /* 'ep' points to the end of available data. */  ep = ndo->ndo_snapend;   if (!ND_TTEST(mh->ip6m_len)) {   /*    * There's not enough captured data to include the    * mobility header length.    *    * Our caller expects us to return the length, however,    * so return a value that will run to the end of the    * captured data.    *    * XXX - \"ip6_print()\" doesn't do anything with the    * returned length, however, as it breaks out of the    * header-processing loop.    */   mhlen = ep - bp;   goto trunc;  }  mhlen = (mh->ip6m_len + 1) << 3;   /* XXX ip6m_cksum */   ND_TCHECK(mh->ip6m_type);  type = mh->ip6m_type;  if (type <= IP6M_MAX && mhlen < ip6m_hdrlen[type]) {   ND_PRINT((ndo, \"(header length %u is too small for type %u)\", mhlen, type));   goto trunc;  }  ND_PRINT((ndo, \"mobility: %s\", tok2str(ip6m_str, \"type-#%u\", type)));  switch (type) {  case IP6M_BINDING_REQUEST:   hlen = IP6M_MINLEN;   break;  case IP6M_HOME_TEST_INIT:  case IP6M_CAREOF_TEST_INIT:  hlen = IP6M_MINLEN;  if (ndo->ndo_vflag) {    ND_TCHECK2(*mh, hlen + 8);  ND_PRINT((ndo, \" %s Init Cookie=%08x:%08x\",  type == IP6M_HOME_TEST_INIT ? \"Home\" : \"Care-of\",  EXTRACT_32BITS(&bp[hlen]),     EXTRACT_32BITS(&bp[hlen + 4])));   }   hlen += 8;   break;  case IP6M_HOME_TEST:  case IP6M_CAREOF_TEST:   ND_TCHECK(mh->ip6m_data16[0]);  ND_PRINT((ndo, \" nonce id=0x%x\", EXTRACT_16BITS(&mh->ip6m_data16[0])));  hlen = IP6M_MINLEN;  if (ndo->ndo_vflag) {    ND_TCHECK2(*mh, hlen + 8);  ND_PRINT((ndo, \" %s Init Cookie=%08x:%08x\",  type == IP6M_HOME_TEST ? \"Home\" : \"Care-of\",  EXTRACT_32BITS(&bp[hlen]),  EXTRACT_32BITS(&bp[hlen + 4])));  }  hlen += 8;  if (ndo->ndo_vflag) {    ND_TCHECK2(*mh, hlen + 8);  ND_PRINT((ndo, \" %s Keygen Token=%08x:%08x\",  type == IP6M_HOME_TEST ? \"Home\" : \"Care-of\",  EXTRACT_32BITS(&bp[hlen]),     EXTRACT_32BITS(&bp[hlen + 4])));   }   hlen += 8;   break;  case IP6M_BINDING_UPDATE:  ND_TCHECK(mh->ip6m_data16[0]);  ND_PRINT((ndo, \" seq#=%u\", EXTRACT_16BITS(&mh->ip6m_data16[0])));  hlen = IP6M_MINLEN;   ND_TCHECK2(*mh, hlen + 1);   if (bp[hlen] & 0xf0)  ND_PRINT((ndo, \" \"));   if (bp[hlen] & 0x80)    ND_PRINT((ndo, \"A\"));   if (bp[hlen] & 0x40)    ND_PRINT((ndo, \"H\"));   if (bp[hlen] & 0x20)    ND_PRINT((ndo, \"L\"));   if (bp[hlen] & 0x10)    ND_PRINT((ndo, \"K\"));  /* Reserved (4bits) */  hlen += 1;  /* Reserved (8bits) */  hlen += 1;   ND_TCHECK2(*mh, hlen + 2);  /* units of 4 secs */  ND_PRINT((ndo, \" lifetime=%u\", EXTRACT_16BITS(&bp[hlen]) << 2));  hlen += 2;  break;  case IP6M_BINDING_ACK:  ND_TCHECK(mh->ip6m_data8[0]);  ND_PRINT((ndo, \" status=%u\", mh->ip6m_data8[0]));  if (mh->ip6m_data8[1] & 0x80)  ND_PRINT((ndo, \" K\"));  /* Reserved (7bits) */  hlen = IP6M_MINLEN;   ND_TCHECK2(*mh, hlen + 2);  ND_PRINT((ndo, \" seq#=%u\", EXTRACT_16BITS(&bp[hlen])));  hlen += 2;   ND_TCHECK2(*mh, hlen + 2);  /* units of 4 secs */  ND_PRINT((ndo, \" lifetime=%u\", EXTRACT_16BITS(&bp[hlen]) << 2));  hlen += 2;   break;  case IP6M_BINDING_ERROR:   ND_TCHECK(mh->ip6m_data8[0]);  ND_PRINT((ndo, \" status=%u\", mh->ip6m_data8[0]));  /* Reserved */  hlen = IP6M_MINLEN;   ND_TCHECK2(*mh, hlen + 16);  ND_PRINT((ndo, \" homeaddr %s\", ip6addr_string(ndo, &bp[hlen])));  hlen += 16;  break;  default:   ND_PRINT((ndo, \" len=%u\", mh->ip6m_len));   return(mhlen);   break;  }  if (ndo->ndo_vflag)   if (mobility_opt_print(ndo, &bp[hlen], mhlen - hlen))    goto trunc;   return(mhlen);   trunc:  ND_PRINT((ndo, \"%s\", tstr));  return(-1); } \n",
      "Original Comment: has a buffer over-read\n",
      "Generated Comment: has a buffer over-read\n",
      "========================================\n",
      "Code: ossl_cipher_pkcs5_keyivgen(int argc, VALUE *argv, VALUE self) {  EVP_CIPHER_CTX *ctx;  const EVP_MD *digest;  VALUE vpass, vsalt, viter, vdigest;  unsigned char key[EVP_MAX_KEY_LENGTH], iv[EVP_MAX_IV_LENGTH], *salt = NULL;  int iter;   rb_scan_args(argc, argv, \"13\", &vpass, &vsalt, &viter, &vdigest);  StringValue(vpass);  if(!NIL_P(vsalt)){  StringValue(vsalt);  if(RSTRING_LEN(vsalt) != PKCS5_SALT_LEN)   ossl_raise(eCipherError, \"salt must be an 8-octet string\");  salt = (unsigned char *)RSTRING_PTR(vsalt);  }  iter = NIL_P(viter) ? 2048 : NUM2INT(viter);  digest = NIL_P(vdigest) ? EVP_md5() : GetDigestPtr(vdigest);  GetCipher(self, ctx);  EVP_BytesToKey(EVP_CIPHER_CTX_cipher(ctx), digest, salt,    (unsigned char *)RSTRING_PTR(vpass), RSTRING_LENINT(vpass), iter, key, iv);  if (EVP_CipherInit_ex(ctx, NULL, NULL, key, iv, -1) != 1)  ossl_raise(eCipherError, NULL);  OPENSSL_cleanse(key, sizeof key);  OPENSSL_cleanse(iv, sizeof iv);  return Qnil;  } \n",
      "Original Comment: applying the same attack one would use in a two-time pad\n",
      "Generated Comment: applying the same attack one would use in a two-time pad\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_test.head(3).iterrows():\n",
    "    print('Code:', row.func_before)\n",
    "    print('Original Comment:', row.explain)\n",
    "    print('Generated Comment:', preds[idx])\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "for i, v in enumerate(zip(preds, references)):\n",
    "    r_ = rouge.compute(predictions=[v[0]], references=[v[1]])\n",
    "    df.append((i, r_['rouge1'], r_['rouge2'], r_['rougeL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.19354838709677422, 0.0689655172413793, 0.19354838709677422),\n",
       " (1, 1.0, 1.0, 1.0),\n",
       " (2, 1.0, 1.0, 1.0),\n",
       " (3, 1.0, 1.0, 1.0),\n",
       " (4, 0.0, 0.0, 0.0),\n",
       " (5, 0.0, 0.0, 0.0),\n",
       " (6, 0.14285714285714288, 0.0, 0.14285714285714288),\n",
       " (7, 1.0, 1.0, 1.0),\n",
       " (8, 1.0, 1.0, 1.0),\n",
       " (9, 0.0, 0.0, 0.0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(df, columns=['id', 'rouge1', 'rouge2', 'rougeL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv(\"root_cause_cpp_12layers.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
