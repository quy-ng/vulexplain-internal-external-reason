{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from project_dataset import load_dataset\n",
    "from code2nl.model import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    model_name = \"neulab/codebert-cpp\"\n",
    "    num_proc = 4\n",
    "    batch_size = 40\n",
    "    max_source_length = 512  \n",
    "    max_target_length = 167 \n",
    "    data_cols = [\"CVE ID\", \"explain\", \"func_before\"]\n",
    "    save_dir = 'tf_board'\n",
    "    epochs = 100\n",
    "    grad_acc_steps = 4\n",
    "    lr = 5e-5\n",
    "    log_freq = 10\n",
    "    local_rank = -1\n",
    "    deepspeed = None\n",
    "    fp16 = False\n",
    "    lr_warmup_steps = 200\n",
    "    weight_decay = 0.05\n",
    "    task = \"impact\"\n",
    "    prefix = 'neulab'\n",
    "    do_lower_case = False\n",
    "    beam_size = 10\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 7032\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 782\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['CVE ID', 'explain', 'func_before', 'processed_func'],\n",
       "        num_rows: 1954\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = ds['train']\n",
    "df_train = df_train.to_pandas()\n",
    "\n",
    "df_val = ds['validation']\n",
    "df_val = df_val.to_pandas()\n",
    "\n",
    "df_test = ds['test']\n",
    "df_test = df_test.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CVE ID</th>\n",
       "      <th>explain</th>\n",
       "      <th>func_before</th>\n",
       "      <th>processed_func</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CVE-2017-15537</td>\n",
       "      <td>read the FPU registers of other processes on t...</td>\n",
       "      <td>int xstateregs_set(struct task_struct *target,...</td>\n",
       "      <td>int xstateregs_set(struct task_struct *target,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CVE-2011-2875</td>\n",
       "      <td>cause a denial of service or possibly have uns...</td>\n",
       "      <td>WebRTCSessionDescriptionDescriptor MockWebRTCP...</td>\n",
       "      <td>WebRTCSessionDescriptionDescriptor\\nMockWebRTC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CVE-2016-4303</td>\n",
       "      <td>overflow a buffer and execute arbitrary code o...</td>\n",
       "      <td>int cJSON_GetArraySize( cJSON *array ) {  cJSO...</td>\n",
       "      <td>int cJSON_GetArraySize(cJSON *array) {\\n  cJSO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           CVE ID                                            explain  \\\n",
       "0  CVE-2017-15537  read the FPU registers of other processes on t...   \n",
       "1   CVE-2011-2875  cause a denial of service or possibly have uns...   \n",
       "2   CVE-2016-4303  overflow a buffer and execute arbitrary code o...   \n",
       "\n",
       "                                         func_before  \\\n",
       "0  int xstateregs_set(struct task_struct *target,...   \n",
       "1  WebRTCSessionDescriptionDescriptor MockWebRTCP...   \n",
       "2  int cJSON_GetArraySize( cJSON *array ) {  cJSO...   \n",
       "\n",
       "                                      processed_func  \n",
       "0  int xstateregs_set(struct task_struct *target,...  \n",
       "1  WebRTCSessionDescriptionDescriptor\\nMockWebRTC...  \n",
       "2  int cJSON_GetArraySize(cJSON *array) {\\n  cJSO...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(args.save_dir, exist_ok=True)\n",
    "os.makedirs(f'tmp_data/{args.task}', exist_ok=True)\n",
    "os.makedirs(f'{args.save_dir}/{args.prefix}_{args.task}', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "df_train['code_tokens'] = df_train.func_before.apply(lambda x: x.split())\n",
    "df_train['docstring_tokens'] = df_train.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/train.jsonl','w') as f:\n",
    "    for _, row in df_train.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "df_val['code_tokens'] = df_val.func_before.apply(lambda x: x.split())\n",
    "df_val['docstring_tokens'] = df_val.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/valid.jsonl','w') as f:\n",
    "    for _, row in df_val.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')\n",
    "\n",
    "df_test['code_tokens'] = df_test.func_before.apply(lambda x: x.split())\n",
    "df_test['docstring_tokens'] = df_test.explain.apply(lambda x: x.split())\n",
    "with open(f'tmp_data/{args.task}/test.jsonl','w') as f:\n",
    "    for _, row in df_test.iterrows():\n",
    "        f.write(json.dumps(row.to_dict()) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = args.lr\n",
    "batch_size = args.batch_size # change depending on the GPU Colab gives you\n",
    "beam_size = args.beam_size\n",
    "source_length = args.max_source_length\n",
    "target_length = args.max_target_length\n",
    "data_dir = 'tmp_data'\n",
    "output_dir = f'{args.save_dir}/{args.prefix}_{args.task}'\n",
    "train_file = f'{data_dir}/{args.task}/train.jsonl'\n",
    "dev_file = f'{data_dir}/{args.task}/valid.jsonl'\n",
    "epochs = args.epochs \n",
    "pretrained_model = args.model_name\n",
    "\n",
    "! python CodeXGLUE/Code-Text/code-to-text/code/run.py \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --do_lower_case \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path {pretrained_model} \\\n",
    "    --train_filename {train_file} \\\n",
    "    --dev_filename {dev_file} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --max_source_length {source_length} \\\n",
    "    --max_target_length {target_length} \\\n",
    "    --beam_size {beam_size} \\\n",
    "    --train_batch_size {batch_size} \\\n",
    "    --eval_batch_size {batch_size} \\\n",
    "    --learning_rate {lr} \\\n",
    "    --num_train_epochs {epochs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "dev_file= f'{data_dir}/{args.task}/valid.jsonl'\n",
    "test_file=f\"{data_dir}/{args.task}/test.jsonl\"\n",
    "test_model=f\"{output_dir}/checkpoint-best-bleu/pytorch_model.bin\" #checkpoint for test\n",
    "\n",
    "! python CodeXGLUE/Code-Text/code-to-text/code/run.py \\\n",
    "    --do_test \\\n",
    "    --model_type roberta \\\n",
    "    --model_name_or_path microsoft/codebert-base \\\n",
    "    --load_model_path {test_model} \\\n",
    "    --dev_filename {dev_file} \\\n",
    "    --test_filename {test_file} \\\n",
    "    --output_dir {output_dir} \\\n",
    "    --max_source_length {source_length} \\\n",
    "    --max_target_length {target_length} \\\n",
    "    --beam_size {beam_size} \\\n",
    "    --eval_batch_size {batch_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_name, do_lower_case=args.do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neulab/codebert-cpp were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-cpp and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): TransformerDecoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerDecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (multihead_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "\n",
    "config = RobertaConfig.from_pretrained(pretrained_model)\n",
    "encoder = RobertaModel.from_pretrained(pretrained_model, config = config)    \n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=config.hidden_size, nhead=config.num_attention_heads)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "model = Seq2Seq(encoder = encoder,decoder = decoder,config=config,\n",
    "                beam_size=beam_size,max_length=target_length,\n",
    "                sos_id=tokenizer.cls_token_id,eos_id=tokenizer.sep_token_id)\n",
    "model.load_state_dict(torch.load(Path(output_dir)/\"checkpoint-best-bleu/pytorch_model.bin\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: static int cg_getattr(const char *path, struct stat *sb) {  struct timespec now;  struct fuse_context *fc = fuse_get_context();  char * cgdir = NULL;  char *fpath = NULL, *path1, *path2;  struct cgfs_files *k = NULL;  const char *cgroup;  const char *controller = NULL;  int ret = -ENOENT;    if (!fc)   return -EIO;   memset(sb, 0, sizeof(struct stat));   if (clock_gettime(CLOCK_REALTIME, &now) < 0)   return -EINVAL;   sb->st_uid = sb->st_gid = 0;  sb->st_atim = sb->st_mtim = sb->st_ctim = now;  sb->st_size = 0;   if (strcmp(path, \"/cgroup\") == 0) {   sb->st_mode = S_IFDIR | 00755;   sb->st_nlink = 2;   return 0;  }   controller = pick_controller_from_path(fc, path);  if (!controller)   return -EIO;  cgroup = find_cgroup_in_path(path);  if (!cgroup) {   /* this is just /cgroup/controller, return it as a dir */   sb->st_mode = S_IFDIR | 00755;   sb->st_nlink = 2;   return 0;  }   get_cgdir_and_path(cgroup, &cgdir, &fpath);   if (!fpath) {   path1 = \"/\";   path2 = cgdir;  } else {   path1 = cgdir;   path2 = fpath;  }   /* check that cgcopy is either a child cgroup of cgdir, or listed in its keys.   * Then check that caller's cgroup is under path if fpath is a child  * cgroup, or cgdir if fpath is a file */  if (is_child_cgroup(controller, path1, path2)) {  if (!caller_is_in_ancestor(fc->pid, controller, cgroup, NULL)) {  /* this is just /cgroup/controller, return it as a dir */  sb->st_mode = S_IFDIR | 00555;    sb->st_nlink = 2;    ret = 0;    goto out;   }   if (!fc_may_access(fc, controller, cgroup, NULL, O_RDONLY)) {    ret = -EACCES;    goto out;   }    sb->st_mode = S_IFDIR | 00755;   k = cgfs_get_key(controller, cgroup, \"tasks\");   if (!k) {    sb->st_uid = sb->st_gid = 0;   } else {    sb->st_uid = k->uid;    sb->st_gid = k->gid;   }   free_key(k);   sb->st_nlink = 2;   ret = 0;   goto out;  }   if ((k = cgfs_get_key(controller, path1, path2)) != NULL) {   sb->st_mode = S_IFREG | k->mode;   sb->st_nlink = 1;   sb->st_uid = k->uid;   sb->st_gid = k->gid;   sb->st_size = 0;   free_key(k);   if (!caller_is_in_ancestor(fc->pid, controller, path1, NULL)) {    ret = -ENOENT;    goto out;   }   if (!fc_may_access(fc, controller, path1, path2, O_RDONLY)) {    ret = -EACCES;    goto out;   }    ret = 0;  }  out:  free(cgdir);  return ret; } \n",
      "Original Comment: a denial of service condition\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "TEXT_TO_SUMMARIZE = df_test.func_before.values[idx]\n",
    "print('Code:', TEXT_TO_SUMMARIZE)\n",
    "print('Original Comment:', df_val.explain.values[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from code2nl.run import convert_examples_to_features, Example\n",
    "\n",
    "def get_preds(df: pd.DataFrame):\n",
    "    ps = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        examples = [\n",
    "            Example(idx, source = row.func_before, target = row.explain)\n",
    "        ]\n",
    "        eval_features = convert_examples_to_features(\n",
    "            examples, tokenizer, args, stage='test'\n",
    "        )\n",
    "        source_ids = torch.tensor(eval_features[0].source_ids, dtype = torch.long).unsqueeze(0).to('cuda')\n",
    "        source_mask = torch.tensor(eval_features[0].source_mask, dtype = torch.long).unsqueeze(0).to('cuda')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(source_ids = source_ids, source_mask = source_mask)  \n",
    "            for pred in preds:\n",
    "                t = pred[0].cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                ps.append(text)\n",
    "    \n",
    "    return ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/29/2023 03:55:08 - WARNING - evaluate.loading -   Using the latest cached version of the module from /data/quy/cache/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sun Jul  9 23:52:56 2023) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0038814544677734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 47,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1954,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5715bbdd041e47fab78aaeba0a19777c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1954 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n",
      "07/29/2023 03:58:59 - INFO - absl -   Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.7052975024475849,\n",
       " 'rouge2': 0.6633614370542575,\n",
       " 'rougeL': 0.6998883135246539,\n",
       " 'rougeLsum': 0.6993792951482853}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_test = df_test.reset_index()\n",
    "preds = get_preds(df_test)\n",
    "references = []\n",
    "for idx, row in df_test.iterrows():\n",
    "    # print('Code:', row.func_before)\n",
    "    # print('Original Comment:', row.explain)\n",
    "    # print('Generated Comment:', preds[idx])\n",
    "    # print('='*40)\n",
    "    references.append(row.explain)\n",
    "\n",
    "results = rouge.compute(predictions=preds, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code: static int cg_getattr(const char *path, struct stat *sb) {  struct timespec now;  struct fuse_context *fc = fuse_get_context();  char * cgdir = NULL;  char *fpath = NULL, *path1, *path2;  struct cgfs_files *k = NULL;  const char *cgroup;  const char *controller = NULL;  int ret = -ENOENT;    if (!fc)   return -EIO;   memset(sb, 0, sizeof(struct stat));   if (clock_gettime(CLOCK_REALTIME, &now) < 0)   return -EINVAL;   sb->st_uid = sb->st_gid = 0;  sb->st_atim = sb->st_mtim = sb->st_ctim = now;  sb->st_size = 0;   if (strcmp(path, \"/cgroup\") == 0) {   sb->st_mode = S_IFDIR | 00755;   sb->st_nlink = 2;   return 0;  }   controller = pick_controller_from_path(fc, path);  if (!controller)   return -EIO;  cgroup = find_cgroup_in_path(path);  if (!cgroup) {   /* this is just /cgroup/controller, return it as a dir */   sb->st_mode = S_IFDIR | 00755;   sb->st_nlink = 2;   return 0;  }   get_cgdir_and_path(cgroup, &cgdir, &fpath);   if (!fpath) {   path1 = \"/\";   path2 = cgdir;  } else {   path1 = cgdir;   path2 = fpath;  }   /* check that cgcopy is either a child cgroup of cgdir, or listed in its keys.   * Then check that caller's cgroup is under path if fpath is a child  * cgroup, or cgdir if fpath is a file */  if (is_child_cgroup(controller, path1, path2)) {  if (!caller_is_in_ancestor(fc->pid, controller, cgroup, NULL)) {  /* this is just /cgroup/controller, return it as a dir */  sb->st_mode = S_IFDIR | 00555;    sb->st_nlink = 2;    ret = 0;    goto out;   }   if (!fc_may_access(fc, controller, cgroup, NULL, O_RDONLY)) {    ret = -EACCES;    goto out;   }    sb->st_mode = S_IFDIR | 00755;   k = cgfs_get_key(controller, cgroup, \"tasks\");   if (!k) {    sb->st_uid = sb->st_gid = 0;   } else {    sb->st_uid = k->uid;    sb->st_gid = k->gid;   }   free_key(k);   sb->st_nlink = 2;   ret = 0;   goto out;  }   if ((k = cgfs_get_key(controller, path1, path2)) != NULL) {   sb->st_mode = S_IFREG | k->mode;   sb->st_nlink = 1;   sb->st_uid = k->uid;   sb->st_gid = k->gid;   sb->st_size = 0;   free_key(k);   if (!caller_is_in_ancestor(fc->pid, controller, path1, NULL)) {    ret = -ENOENT;    goto out;   }   if (!fc_may_access(fc, controller, path1, path2, O_RDONLY)) {    ret = -EACCES;    goto out;   }    ret = 0;  }  out:  free(cgdir);  return ret; } \n",
      "Original Comment: gain elevated privileges on the system\n",
      "Generated Comment: gain elevated privileges on the system\n",
      "========================================\n",
      "Code: void BufferQueueConsumer::dump(String8& result, const char* prefix) const {  const IPCThreadState* ipc = IPCThreadState::self();  const pid_t pid = ipc->getCallingPid();  const uid_t uid = ipc->getCallingUid();  if ((uid != AID_SHELL)  && !PermissionCache::checkPermission(String16(   \"android.permission.DUMP\"), pid, uid)) {  result.appendFormat(\"Permission Denial: can't dump BufferQueueConsumer \"  \"from pid=%d, uid=%d\\n\", pid, uid);  } else {  mCore->dump(result, prefix);  } } \n",
      "Original Comment: obtain sensitive information from process memory and bypass protection mechanism\n",
      "Generated Comment: gain elevated privileges on the system\n",
      "========================================\n",
      "Code:  TemplateURLRef::SearchTermsArgs::ContextualSearchParams::ContextualSearchParams(  int version,  const std::string& selection,  const std::string& base_page_url,  int now_on_tap_version)  : version(version),  start(base::string16::npos),  end(base::string16::npos),  selection(selection),  base_page_url(base_page_url),  now_on_tap_version(now_on_tap_version) {} \n",
      "Original Comment: cause a denial of service or possibly have other impact\n",
      "Generated Comment: cause a denial of service or possibly have other impact\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df_test.head(3).iterrows():\n",
    "    print('Code:', row.func_before)\n",
    "    print('Original Comment:', row.explain)\n",
    "    print('Generated Comment:', preds[idx])\n",
    "    print('='*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
